# -*- coding: utf-8 -*-
"""D_ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DxY6dw4HTpC7V-qbXadaY31lfjBWCmIa

# Importing Lbraries and Data

Libraries
"""




import json
import time
from datetime import datetime, timedelta, timezone
import pandas as pd
from matplotlib import pyplot as plt

"""Data"""

with open('D:/ddos/traffic_with_ddos_and_peak.json', 'r') as f:
  json_log = json.load(f)

df = pd.DataFrame(json_log)

"""# Preprocessing

Create relative timestamp (UNIX timestamp)
"""

def UNIX_time(df):

  # Convert timestamp to datetime
  df['timestamp'] = pd.to_datetime(df['timestamp'], format='ISO8601')

  # Convert the 'date_time' column to UNIX timestamp
  df['unix_timestamp'] = df['timestamp'].astype('int64') // 10**9  # Divide by 10^9 to get seconds

  return df

df = UNIX_time(df)

"""Sort according to time"""

df.sort_values(by='unix_timestamp', inplace=True) # for training data only

"""Create Time Windows"""

def create_windows(df):
    # Define the size of each window
    window_size = 1  # 2 seconds

    # Create empty lists to store counts and differences
    traffic_in_window_1 = []
    traffic_in_window_2 = []
    traffic_in_window_3 = []
    traffic_in_window_4 = []

    # Define a sliding window
    start_time = df['unix_timestamp'].min()
    previous_count = None  # Variable to store the count of the previous window

    while start_time + (4 * window_size) <= df['unix_timestamp'].max():
        # Define the time windows
        first_window = (start_time, start_time + window_size)
        second_window = (start_time + window_size, start_time + 2 * window_size)
        third_window = (start_time + 2 * window_size, start_time + 3 * window_size)
        fourth_window = (start_time + 3 * window_size, start_time + 4 * window_size)

        # Count the number of requests in each window
        count_1st = df[(df['unix_timestamp'] >= first_window[0]) & (df['unix_timestamp'] < first_window[1])].shape[0]
        count_2nd = df[(df['unix_timestamp'] >= second_window[0]) & (df['unix_timestamp'] < second_window[1])].shape[0]
        count_3rd = df[(df['unix_timestamp'] >= third_window[0]) & (df['unix_timestamp'] < third_window[1])].shape[0]
        count_4th = df[(df['unix_timestamp'] >= fourth_window[0]) & (df['unix_timestamp'] < fourth_window[1])].shape[0]

        # Calculate differences
        if previous_count is None:
            diff_1st = count_1st  # Or use a default value
        else:
            diff_1st = count_1st - previous_count

        diff_2nd = count_2nd - count_1st
        diff_3rd = count_3rd - count_2nd
        diff_4th = count_4th - count_3rd

        # Append the differences to the lists
        traffic_in_window_1.append(diff_1st)
        traffic_in_window_2.append(diff_2nd)
        traffic_in_window_3.append(diff_3rd)
        traffic_in_window_4.append(diff_4th)

        # Update the previous_count and move the window by 2 seconds (sliding mechanism)
        previous_count = count_1st
        start_time += 1

    # Create a new DataFrame with the collected data
    feature_df = pd.DataFrame({
        'traffic in window 1': traffic_in_window_1,
        'traffic in window 2': traffic_in_window_2,
        'traffic in window 3': traffic_in_window_3,
        'traffic in window 4': traffic_in_window_4
    })
    return feature_df

windows_df = create_windows(df)

"""Feature Scaling"""

from sklearn.preprocessing import StandardScaler

def scale_windows(df):
    # Initialize the scaler
    scaler = StandardScaler()
    #scaler = MinMaxScaler(feature_range=(0, 1))

    # Define which columns to scale
    feature_columns = df.columns  # Adjust this as necessary

    # Fit and transform the feature columns
    scaled_features = scaler.fit_transform(df[feature_columns])

    # Create a DataFrame from the scaled features
    scaled_df = pd.DataFrame(scaled_features, columns=feature_columns)

    return scaled_df

features_df = scale_windows(windows_df)

"""# Model Training (Unsupervised)

KMeans
"""

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, n_init=10, random_state=42)  # 3 clusters: benign, hike, downward; 10 initialisations to choose the best from
kmeans.fit(features_df)

"""# Detector AI"""

def count_test_features(df, timestamp_column='timestamp'):

    # Convert UNIX time to datetime for easier manipulation
    #df[timestamp_column] = pd.to_datetime(df[timestamp_column], unit='s')
    df[timestamp_column] = pd.to_datetime(df[timestamp_column]).dt.floor('S')


    # Get the first timestamp and calculate subsequent timestamps
    first_timestamp = df[timestamp_column].min()
    timestamps = {
        'traffic in window 1': first_timestamp,
        'traffic in window 2': first_timestamp + pd.Timedelta(seconds=1),
        'traffic in window 3': first_timestamp + pd.Timedelta(seconds=2),
        'traffic in window 4': first_timestamp + pd.Timedelta(seconds=3)
    }
    
    # Count rows for each timestamp
    counts = {}
    for label, time_point in timestamps.items():
        counts[label] = df[df[timestamp_column] == time_point].shape[0]

    # Create DataFrame from counts
    feature_count = pd.DataFrame(counts, index=[0])
    
    return feature_count

# Initialize previous_feature_df to store previous values
previous_feature_df = pd.DataFrame({
    'traffic in window 1': [0],
    'traffic in window 2': [0],
    'traffic in window 3': [0],
    'traffic in window 4': [0]
})

# Function to compute the difference between current and previous values
def compute_difference(test_feature_df):
    global previous_feature_df  # Keep track of the previous values globally

    # Compute the difference between the current and previous feature DataFrame
    difference_df = test_feature_df - previous_feature_df

    # Update previous_feature_df with current values for the next iteration
    previous_feature_df = test_feature_df.copy()

    return difference_df


# Function to load the log file and parse the JSON list
def load_log_file(file_path):
    with open(file_path, 'r') as file:
        try:
            log_data = json.load(file)
        except json.JSONDecodeError:
            log_data = []
    return log_data

# Function to filter JSON objects within the last 3 seconds
def get_recent_entries(log_data, current_time, seconds=4):
    threshold_time = current_time - timedelta(seconds=seconds)
    return [entry for entry in log_data if datetime.fromisoformat(entry['timestamp'].replace('Z', '+00:00')) > threshold_time]

# Global placeholders
placeholders = {"a": None, "b": None, "c": None, "d": None}

# Function to load the log file and parse the JSON list
def load_log_file(file_path):
    with open(file_path, 'r') as file:
        try:
            log_data = json.load(file)
        except json.JSONDecodeError:
            log_data = []
    return log_data

# Function to filter JSON objects within the last 3 seconds
def get_recent_entries(log_data, current_time, seconds=3):
    threshold_time = current_time - timedelta(seconds=seconds)
    return [entry for entry in log_data if datetime.fromisoformat(entry['timestamp'].replace('Z', '+00:00')) > threshold_time]

# Main logic for tracking placeholders and updating j_test
def update_j_test(file_path):
    global placeholders  # Declare the global placeholders

    # Load the latest log data
    log_data = load_log_file(file_path)

    # Get current time with UTC timezone
    current_time = datetime.now(timezone.utc)

    # Update placeholders after 1 second
    placeholders["a"] = placeholders["b"]
    placeholders["b"] = placeholders["c"]
    placeholders["c"] = placeholders["d"]
    placeholders["d"] = len(log_data)  # Placeholder d at the new bottom

    # Get all JSON objects between placeholders a and d
    if placeholders["a"] is not None:
        recent_data = log_data[placeholders["a"]:placeholders["d"]]
    else:
        # On the first run, there are no placeholders a/b/c yet
        recent_data = get_recent_entries(log_data, current_time)

    # Update j_test with the recent data
    j_test = recent_data
    
    return j_test





# Main loop to check for new log entries and predict clusters
while True:

    # Call the update function with the correct path to your log file
    j_test = update_j_test('D:/ddos/Main/logs/logs.json')
    
    # Check if j_test is empty
    if not j_test:
        print("No new entries found, waiting...")
        time.sleep(1)
        continue  # Skip the rest of the loop and check again

    # Convert j_test to DataFrame
    test_df = pd.DataFrame(j_test)
    
    # Apply UNIX time conversion
    #test_df = UNIX_time(test_df)
    test_df['timestamp'] = pd.to_datetime(test_df['timestamp'], format='ISO8601')

    # Generate features from the test data
    test_feature_df = count_test_features(test_df)
    
    # Compute the difference
    difference_df = compute_difference(test_feature_df)

    # Eliminate NaN values
    difference_df.fillna(0, inplace=True)

    # Sleep for 1 second before checking the file again
    
     # Create scaled test features
    scaled_test_features = scale_windows(difference_df)

    # Predict the cluster label using the GMM model
    cluster_label = kmeans.predict(difference_df)

    # Output the predicted cluster
    print(f"Predicted cluster label: {cluster_label}")
    
    time.sleep(1)

